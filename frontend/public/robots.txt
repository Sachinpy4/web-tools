# Web Tools - Robots.txt Configuration
# This file controls how search engines crawl and index your site

# Allow all search engines to crawl public content
User-agent: *

# DISALLOW: Admin, backend, and private areas
Disallow: /dashboard/
Disallow: /settings/
Disallow: /login
Disallow: /debug
Disallow: /api/

# DISALLOW: Old URLs that cause 404 errors
Disallow: /download/
Disallow: /Web

# DISALLOW: Common system directories and file types
Disallow: /_next/
Disallow: /_vercel/
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /*.log$
Disallow: /*.env$
Disallow: /*.sql$
Disallow: /*.db$
Disallow: /*.bak$
Disallow: /*.config$
Disallow: /*.ini$

# DISALLOW: Static assets that don't need indexing
Disallow: /*.woff$
Disallow: /*.woff2$
Disallow: /*.ttf$
Disallow: /*.eot$
Disallow: /*.ico$
Disallow: /*.css$
Disallow: /*.js$
Disallow: /*.map$

# DISALLOW: Search and filter parameters to avoid duplicate content
Disallow: /*?search=*
Disallow: /*?filter=*
Disallow: /*?sort=*
Disallow: /*?page=*
Disallow: /*?category=*
Disallow: /*?tag=*

# CRAWL DELAY: Be respectful to server resources
Crawl-delay: 1

# SITEMAP: Point to sitemap location
Sitemap: https://toolscandy.com/sitemap.xml
# SPECIFIC RULES FOR MAJOR SEARCH ENGINES

# Google Bot - Allow faster crawling
User-agent: Googlebot
Crawl-delay: 0.5

# Bing Bot
User-agent: Bingbot
Crawl-delay: 1

# Block aggressive crawlers that might overload your server
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Block AI training bots (optional - uncomment if you want to block them)
# User-agent: GPTBot
# Disallow: /

# User-agent: ChatGPT-User
# Disallow: /

# User-agent: CCBot
# Disallow: /

# User-agent: anthropic-ai
# Disallow: /

# User-agent: Claude-Web
# Disallow: /